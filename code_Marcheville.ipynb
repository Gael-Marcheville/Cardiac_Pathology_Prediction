{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Challenge IMA 205 : Cardiac Pathology Prediction \n",
    "### Gaël Marcheville, 2nd year student at Télécom Paris "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Readme\n",
    "\n",
    "This notebook is meant to be run on Jupyter Notebook. I only tested it on VSCode, but it should work on any other IDE. The code is written in Python 3.11. The notebook should be run at the root of the project, so that the data is in the same folder as the notebook (Train and Test folders). Please contact me if you have any problem running the notebook.\n",
    "\n",
    "It create a file called `submission.csv` in the same folder as the notebook. This file is the one to be submitted on the challenge website.\n",
    "\n",
    "## 1. Import\n",
    "\n",
    "Below are all the libraries used for this project. The main libraries are pandas for data processing, numpy and scipy for calculations and statistics, and scikit-learn for machine learning algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV version :  4.7.0\n",
      "NIB version :  5.1.0\n",
      "Numpy version :  1.23.5\n",
      "Pandas version :  2.0.1\n",
      "Scipy version :  2.0.1\n",
      "Scikit-learn version :  2.0.1\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "print(\"CV version : \",cv.__version__) # 4.7.0 for me\n",
    "import nibabel as nib\n",
    "print(\"NIB version : \",nib.__version__) # 5.1.0 for me\n",
    "import numpy as np\n",
    "print(\"Numpy version : \",np.__version__) # 1.23.5 for me    \n",
    "import pandas as pd\n",
    "print(\"Pandas version : \",pd.__version__) # 2.0.1 for me\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from scipy.stats import kurtosis, skew\n",
    "print(\"Scipy version : \",pd.__version__) # 2.0.1 for me\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "print(\"Scikit-learn version : \",pd.__version__) # 2.0.1 for me"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = pd.read_csv('./metaDataTrain.csv', sep = ',')\n",
    "dataTest = pd.read_csv('./metaDataTest.csv', sep = ',')\n",
    "\n",
    "Id = dataTest['Id']\n",
    "class_names = ['0','1','2','3','4'] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithms to compute features\n",
    "### a. Compute Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vol(binary_img, voxel_size = (1,1,1)):\n",
    "    binary_img = binary_img.astype(np.uint8)\n",
    "    return np.sum(binary_img) * np.prod(voxel_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Segment the left ventricle for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_contour(binary_img):\n",
    "    mask = np.zeros(binary_img.shape)\n",
    "    for i in range (binary_img.shape[2]):\n",
    "        mask[:,:,i] = binary_fill_holes(binary_img[:,:,i]).astype(int)\n",
    "        mask[:,:,i] -= binary_img[:,:,i]\n",
    "    return mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Compute the thickness, circularity and circumference of the myocardium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thickness_myo(binary_img, voxel_size = (1,1,1)):\n",
    "    binary_img = binary_img.astype(np.uint8)\n",
    "    thickness = np.zeros(binary_img.shape[2])\n",
    "    for k in range (binary_img.shape[2]):\n",
    "        dist_transform = cv.distanceTransform(binary_img[:,:,k], cv.DIST_L2, 5) * (np.sqrt(voxel_size[0] + voxel_size[1]))\n",
    "        thickness[k] = (dist_transform.max() - dist_transform.min()) \n",
    "    return np.mean(thickness), np.std(thickness), np.max(thickness), np.min(thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circularity(binary_img, voxel_size = (1,1,1)):\n",
    "    binary_img = binary_img.astype(np.uint8)\n",
    "    circularity = np.zeros((binary_img.shape[2],))\n",
    "    for k in range(binary_img.shape[2]):\n",
    "        contours, hierarchy = cv.findContours(binary_img[:,:,k], cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) > 0:\n",
    "            cnt = contours[0]\n",
    "            area = cv.contourArea(cnt)\n",
    "            perimeter = cv.arcLength(cnt,True) * (np.sqrt(voxel_size[0] + voxel_size[1]))\n",
    "            if perimeter != 0:\n",
    "                circularity[k] = 4*np.pi*area/(perimeter*perimeter)\n",
    "            else:\n",
    "                circularity[k] = 0\n",
    "    return np.mean(circularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circumference_myo(binary_img, voxel_size = (1,1,1)):\n",
    "    binary_img = binary_img.astype(np.uint8)\n",
    "    circumference = np.zeros(binary_img.shape[2])\n",
    "    for k in range (binary_img.shape[2]):\n",
    "        contours, _ = cv.findContours(binary_img[:,:,k], cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            circumference[k] = cv.arcLength(cnt, True) * (np.sqrt(voxel_size[0] + voxel_size[1]))\n",
    "    return np.mean(circumference), np.max(circumference), np.min(circumference)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add the features to the dataframe\n",
    "\n",
    "The following code adds the features to the dataframe. It takes as input the dataframe, and returns the dataframe with the features. It uses the functions defined above. XTrainImg and XTestImg are the images of the training and test set. XTrain and XTest are whole features of the training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain = dataTrain.drop(['Category','Id'], axis=1)\n",
    "XTest = dataTest.drop(['Id'], axis=1)\n",
    "yTrain = dataTrain['Category']\n",
    "\n",
    "shape = nib.load('.\\\\Train\\\\001\\\\001_ED.nii').get_fdata().shape\n",
    "\n",
    "XTrainImg = np.empty((4, 100), dtype=object)\n",
    "XTestImg = np.empty((4, 100), dtype=object)\n",
    "\n",
    "#adding data for train\n",
    "for k in range (0,XTrain.shape[0]):\n",
    "    \n",
    "    k_str = '{:03d}'.format(k + 1)\n",
    "    ED = '.\\\\Train\\\\' + k_str + '\\\\' + k_str + '_ED.nii'\n",
    "    ED_seg = '.\\\\Train\\\\' + k_str + '\\\\' + k_str + '_ED_seg.nii'\n",
    "    ES = '.\\\\Train\\\\' + k_str + '\\\\' + k_str + '_ES.nii'\n",
    "    ES_seg = '.\\\\Train\\\\' + k_str + '\\\\' + k_str + '_ES_seg.nii'\n",
    "\n",
    "    XTrainImg[0,k] = nib.load(ED)\n",
    "    XTrainImg[1,k] = nib.load(ED_seg)\n",
    "    XTrainImg[2,k] = nib.load(ES)\n",
    "    XTrainImg[3,k] = nib.load(ES_seg)\n",
    "\n",
    "    # Instant features\n",
    "    # ED \n",
    "    data = XTrainImg[1,k].get_fdata()\n",
    "    zoom = np.array(XTrainImg[1,k].header.get_zooms())\n",
    "    XTrain.loc[k,'rv_ED'] = compute_vol((data == 1), zoom) #1 - Left ventricle cavity\n",
    "    XTrain.loc[k,'myo_ED'] = compute_vol((data == 2), zoom) #2 - Myocardium\n",
    "    XTrain.loc[k,'lv_ED'] = compute_vol((data == 3), zoom) #3 - Right ventricle cavity\n",
    "    XTrain.loc[k,'thick_ED_mean'], XTrain.loc[k,'thick_ED_std'], XTrain.loc[k,'thick_ED_max'], XTrain.loc[k,'thick_ED_min'] = thickness_myo((data == 2),zoom) #4 - Myocardium thickness Mean\n",
    "    XTrain.loc[k,'circul_ED'] = circularity((data == 2),zoom) #5 - Myocardium circularity\n",
    "    XTrain.loc[k,'circum_ED_mean'], XTrain.loc[k,'circum_ED_max'], XTrain.loc[k,'circum_ED_min'] = circumference_myo((data == 2),zoom) #6 - Myocardium circumference\n",
    "    XTrain.loc[k,'r_rv_lv_ED'] = XTrain.loc[k,'rv_ED'] / XTrain.loc[k,'lv_ED'] #7 - Ratio between RV and LV\n",
    "    XTrain.loc[k, 'r_myo_lv_ED'] = XTrain.loc[k,'myo_ED'] / XTrain.loc[k,'lv_ED'] #8 - Ratio between Myocardium and LV\n",
    "\n",
    "    # ES\n",
    "    data = XTrainImg[3,k].get_fdata()\n",
    "    zoom = np.array(XTrainImg[3,k].header.get_zooms())\n",
    "    XTrain.loc[k,'rv_ES'] = compute_vol((data == 1), zoom) #1 - Left ventricle cavity\n",
    "    XTrain.loc[k,'myo_ES'] = compute_vol((data == 2), zoom) #2 - Myocardium\n",
    "    XTrain.loc[k,'lv_ES'] = compute_vol((data == 3), zoom) #3 - Right ventricle cavity\n",
    "    XTrain.loc[k,'thick_ES_mean'], XTrain.loc[k,'thick_ES_std'], XTrain.loc[k,'thick_ES_max'], XTrain.loc[k,'thick_ES_min'] = thickness_myo((data == 2), zoom) #4 - Myocardium thickness\n",
    "    XTrain.loc[k,'circul_ES'] = circularity((data == 2), zoom) #5 - Myocardium circularity\n",
    "    XTrain.loc[k,'circum_ES_mean'], XTrain.loc[k,'circum_ES_max'], XTrain.loc[k,'circum_ES_min'] = circumference_myo((data == 2), zoom) #6 - Myocardium circumference\n",
    "    XTrain.loc[k,'r_rv_lv_ES'] = XTrain.loc[k,'rv_ES'] / XTrain.loc[k,'lv_ES'] #7 - Ratio between RV and LV\n",
    "    XTrain.loc[k, 'r_myo_lv_ES'] = XTrain.loc[k,'myo_ES'] / XTrain.loc[k,'lv_ES'] #8 - Ratio between Myocardium and LV\n",
    "\n",
    "    # Dynamic features\n",
    "    #add Ejection Fraction\n",
    "    XTrain.loc[k,'lv_EF'] = (XTrain.loc[k,'lv_ED'] - XTrain.loc[k,'lv_ES']) / XTrain.loc[k,'lv_ED']\n",
    "    XTrain.loc[k,'rv_EF'] = (XTrain.loc[k,'rv_ED'] - XTrain.loc[k,'rv_ES']) / XTrain.loc[k,'rv_ED']\n",
    "\n",
    "    #median, standard deviation, kurtosis, skewness of the volume\n",
    "    myo_ED = (XTrainImg[1,k].get_fdata() == 2).astype(np.uint8)\n",
    "    myo_ES = (XTrainImg[3,k].get_fdata() == 2).astype(np.uint8)\n",
    "    elem_vol = zoom[0] * zoom[1] * zoom[2]\n",
    "    XTrain.loc[k,'myo_vol_median'] = np.median(myo_ED - myo_ES) * elem_vol\n",
    "    XTrain.loc[k,'myo_vol_std'] = np.std(myo_ED - myo_ES) * elem_vol\n",
    "    XTrain.loc[k,'myo_vol_kurtosis'] = kurtosis(myo_ED - myo_ES, axis=None)\n",
    "    XTrain.loc[k,'myo_vol_skewness'] = skew(myo_ED - myo_ES, axis=None)\n",
    "\n",
    "labels = XTrain.columns.values.tolist()\n",
    "\n",
    "#adding data for test\n",
    "for k in range (0,XTest.shape[0]):\n",
    "    \n",
    "    k_str = '{:03d}'.format(k + XTrain.shape[0] + 1)\n",
    "    ED = '.\\\\Test\\\\' + k_str + '\\\\' + k_str + '_ED.nii'\n",
    "    ED_seg = '.\\\\Test\\\\' + k_str + '\\\\' + k_str + '_ED_seg.nii'\n",
    "    ES = '.\\\\Test\\\\' + k_str + '\\\\' + k_str + '_ES.nii'\n",
    "    ES_seg = '.\\\\Test\\\\' + k_str + '\\\\' + k_str + '_ES_seg.nii'\n",
    "    \n",
    "    XTestImg[0,k] = nib.load(ED)\n",
    "    XTestImg[1,k] = nib.load(ED_seg)\n",
    "    XTestImg[2,k] = nib.load(ES)\n",
    "    XTestImg[3,k] = nib.load(ES_seg)\n",
    "\n",
    "    # ED\n",
    "    data = XTestImg[1,k].get_fdata()\n",
    "    zoom = np.array(XTestImg[1,k].header.get_zooms())\n",
    "    #add seg\n",
    "    data += 3*mask_contour(data == 2)\n",
    "    XTest.loc[k,'rv_ED'] = compute_vol((data == 1),zoom) #1 - Left ventricle cavity\n",
    "    XTest.loc[k,'myo_ED'] = compute_vol((data == 2), zoom) #2 - Myocardium\n",
    "    XTest.loc[k,'lv_ED'] = compute_vol((data == 3), zoom) #3 - Right ventricle cavity\n",
    "    XTest.loc[k,'thick_ED_mean'], XTest.loc[k,'thick_ED_std'], XTest.loc[k,'thick_ED_max'], XTest.loc[k,'thick_ED_min'] = thickness_myo((data == 2), zoom) #4 - Myocardium thickness Mean\n",
    "    XTest.loc[k,'circul_ED'] = circularity((data == 2), zoom) #5 - Myocardium circularity\n",
    "    XTest.loc[k,'circum_ED_mean'], XTest.loc[k,'circum_ED_max'], XTest.loc[k,'circum_ED_min'] = circumference_myo((data == 2), zoom) #6 - Myocardium circumference\n",
    "    XTest.loc[k,'r_rv_lv_ED'] = XTest.loc[k,'rv_ED'] / XTest.loc[k,'lv_ED'] #7 - Ratio between RV and LV\n",
    "    XTest.loc[k, 'r_myo_lv_ED'] = XTest.loc[k,'myo_ED'] / XTest.loc[k,'lv_ED'] #8 - Ratio between Myocardium and LV\n",
    "\n",
    "    # ES\n",
    "    data = XTestImg[3,k].get_fdata()\n",
    "    zoom = np.array(XTestImg[3,k].header.get_zooms())\n",
    "    #add seg\n",
    "    data += 3*mask_contour(data == 2)\n",
    "    XTest.loc[k,'rv_ES'] = compute_vol((data == 1), zoom) #1 - Left ventricle cavity\n",
    "    XTest.loc[k,'myo_ES'] = compute_vol((data == 2), zoom) #2 - Myocardium\n",
    "    XTest.loc[k,'lv_ES'] = compute_vol((data == 3), zoom) #3 - Right ventricle cavity\n",
    "    XTest.loc[k,'thick_ES_mean'], XTest.loc[k,'thick_ES_std'], XTest.loc[k,'thick_ES_max'], XTest.loc[k,'thick_ES_min'] = thickness_myo((data == 2), zoom) #4 - Myocardium thickness\n",
    "    XTest.loc[k,'circul_ES'] = circularity((data == 2), zoom) #5 - Myocardium circularity\n",
    "    XTest.loc[k,'circum_ES_mean'], XTest.loc[k,'circum_ES_max'], XTest.loc[k,'circum_ES_min'] = circumference_myo((data == 2), zoom) #6 - Myocardium circumference\n",
    "    XTest.loc[k,'r_rv_lv_ES'] = XTest.loc[k,'rv_ES'] / XTest.loc[k,'lv_ES'] #7 - Ratio between RV and LV\n",
    "    XTest.loc[k, 'r_myo_lv_ES'] = XTest.loc[k,'myo_ES'] / XTest.loc[k,'lv_ES'] #8 - Ratio between Myocardium and LV\n",
    "    \n",
    "    # Ejection Fraction\n",
    "    XTest.loc[k,'lv_EF'] = (XTest.loc[k,'lv_ED'] - XTest.loc[k,'lv_ES']) / XTest.loc[k,'lv_ED']\n",
    "    XTest.loc[k,'rv_EF'] = (XTest.loc[k,'rv_ED'] - XTest.loc[k,'rv_ES']) / XTest.loc[k,'rv_ED']\n",
    "\n",
    "    #median, standard deviation, kurtosis, skewness of the volume\n",
    "    myo_ED = (XTestImg[1,k].get_fdata() == 2).astype(np.uint8)\n",
    "    myo_ES = (XTestImg[3,k].get_fdata() == 2).astype(np.uint8)\n",
    "    elem_vol = zoom[0] * zoom[1] * zoom[2]\n",
    "    XTest.loc[k,'myo_vol_median'] = np.median(myo_ED - myo_ES) * elem_vol\n",
    "    XTest.loc[k,'myo_vol_std'] = np.std(myo_ED - myo_ES) * elem_vol\n",
    "    XTest.loc[k,'myo_vol_kurtosis'] = kurtosis(myo_ED - myo_ES, axis=None)\n",
    "    XTest.loc[k,'myo_vol_skewness'] = skew(myo_ED - myo_ES, axis=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithms used for prediction\n",
    "### a. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = ['rv_ED', \n",
    "     'myo_ED', \n",
    "     'lv_ED', \n",
    "     'thick_ED_mean', \n",
    "     'thick_ED_max',\n",
    "     'r_rv_lv_ED',\n",
    "     'r_myo_lv_ED',\n",
    "     'rv_ES',\n",
    "     'lv_ES',\n",
    "     'thick_ES_mean',\n",
    "     'thick_ES_max',\n",
    "     'r_rv_lv_ES',\n",
    "     'r_myo_lv_ES',\n",
    "     'rv_EF']\n",
    "\n",
    "x_train = XTrain\n",
    "y_train = yTrain\n",
    "x_test = XTest\n",
    "\n",
    "Forest = RandomForestClassifier(n_estimators=1000, random_state = 0)\n",
    "Forest.fit(x_train[L], y_train)\n",
    "scores = cross_val_score(Forest, x_train[L], y_train, cv=7)\n",
    "y_pred = Forest.predict(x_test[L])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Gradient Boosting Classifier to classified MINF and DCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = ['rv_ED', \n",
    "     'myo_ED', \n",
    "     'lv_ED', \n",
    "     'r_rv_lv_ED',\n",
    "     'r_myo_lv_ED',\n",
    "     'rv_ES',\n",
    "     'myo_ES',\n",
    "     'lv_ES',\n",
    "     'thick_ES_mean',\n",
    "     'r_rv_lv_ES',\n",
    "     'r_myo_lv_ES',\n",
    "     'lv_EF',\n",
    "     'rv_EF']\n",
    "\n",
    "# Select samples classified as MINF and DCM\n",
    "x_train_2 = x_train[(y_train == 1) | (y_train == 2)]\n",
    "y_train_2 = y_train[(y_train == 1) | (y_train == 2)]\n",
    "x_test_2 = x_test[(y_pred == 1) | (y_pred == 2)]\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_2[L])\n",
    "x_train_2.loc[:, L] = scaler.transform(x_train_2[L])\n",
    "x_test_2.loc[:, L] = scaler.transform(x_test_2[L])\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=1, random_state=34)\n",
    "model.fit(x_train_2[L], y_train_2)\n",
    "scores = cross_val_score(model, x_train_2[L], y_train_2, cv=7)\n",
    "y_pred_2 = model.predict(x_test_2[L])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Merge the classification of Gradient Boosting Classifier and Random Forest Classifier, and export the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the classifiers\n",
    "y_pred[(y_pred == 1) | (y_pred == 2)] = y_pred_2\n",
    "\n",
    "# Export the predictions to a csv file for submission\n",
    "y_pred = y_pred.astype(int)\n",
    "df = pd.DataFrame({'Id': Id, 'Category': y_pred})\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other algorithms tested\n",
    "\n",
    "### a. Multi-Level perceptron (MLP) (not used in the Final Model)\n",
    "\n",
    "The first is a MLP inspired by one of the articles. It doesn't give good results, but I keep it as a trace of my research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLP architecture\n",
    "mlp_architecture = (32, 32, 32, 32)\n",
    "mlp_batch_size = 40\n",
    "mlp_epochs = 4000\n",
    "mlp_learning_rate = 5e-4\n",
    "\n",
    "L = ['rv_ED', \n",
    "     'myo_ED', \n",
    "     'lv_ED', \n",
    "     'thick_ED_mean', \n",
    "     'thick_ED_max',\n",
    "     'r_rv_lv_ED',\n",
    "     'r_myo_lv_ED',\n",
    "     'rv_ES',\n",
    "     'lv_ES',\n",
    "     'thick_ES_mean',\n",
    "     'thick_ES_max',\n",
    "     'r_rv_lv_ES',\n",
    "     'r_myo_lv_ES',\n",
    "     'rv_EF']\n",
    "\n",
    "# Set up training and testing data with a 75/25 split\n",
    "x_train, x_test, y_train, y_test = train_test_split(XTrain[L], yTrain, test_size=0.2, random_state=42)\n",
    "\n",
    "#convert to numpy array\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# Initialize MLP and Random Forest models\n",
    "mlps = []\n",
    "for i in range(50):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=mlp_architecture,\n",
    "                        activation='relu',\n",
    "                        alpha=0.0001,\n",
    "                        batch_size=mlp_batch_size,\n",
    "                        learning_rate_init=mlp_learning_rate,\n",
    "                        max_iter=mlp_epochs,\n",
    "                        shuffle=True,\n",
    "                        random_state=i)\n",
    "    mlps.append(mlp)\n",
    "\n",
    "# Train each MLP on a random subset of the training data\n",
    "for i in range(50):\n",
    "    # Randomly select 2/3 of the training data\n",
    "    subset = np.random.choice(x_train.shape[0], size=int(0.66 * x_train.shape[0]), replace=False)\n",
    "    x_subset = x_train[subset]\n",
    "    y_subset = y_train[subset]\n",
    "    # Only present a random subset of 2/3 of the features to each MLP\n",
    "    features_subset = np.random.choice(x_train.shape[1], size=int(0.66 * x_train.shape[1]), replace=False)\n",
    "    x_subset = x_subset[:, features_subset]\n",
    "    # Scale the data and fit the MLP\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(x_subset)\n",
    "    mlps[i].fit(x_scaled, y_subset)\n",
    "\n",
    "# Evaluate MLP ensemble on test data\n",
    "mlp_predictions = []\n",
    "for i in range(50):\n",
    "    # Scale the test data\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(x_test)\n",
    "    # Only use a random subset of 2/3 of the features\n",
    "    x_scaled = x_scaled[:, features_subset]\n",
    "    # Make predictions\n",
    "    mlp_predictions.append(mlps[i].predict(x_scaled))\n",
    "\n",
    "# Ensemble predictions\n",
    "mlp_predictions = np.array(mlp_predictions)\n",
    "mlp_predictions = np.swapaxes(mlp_predictions, 0, 1)\n",
    "ensemble_predictions = []\n",
    "for i in range(mlp_predictions.shape[0]):\n",
    "    ensemble_predictions.append(np.argmax(np.bincount(mlp_predictions[i])))\n",
    "ensemble_predictions = np.array(ensemble_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
